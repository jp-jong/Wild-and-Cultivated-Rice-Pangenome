{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd6c53e-e113-4d5d-84d5-49b222745c9c",
   "metadata": {},
   "source": [
    "This part of the code is the get_bialsv.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74beb0d-3f50-42e2-a5eb-8749bf601717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class svid:\n",
    "    \"\"\"\n",
    "    An object to store the sv information\n",
    "    \"\"\"\n",
    "    chromo: str\n",
    "    pos: int\n",
    "    svtype: str\n",
    "    reflen: int\n",
    "    nonreflen: int\n",
    "    sourcenode: str\n",
    "    bubref: str\n",
    "    bubnonref: str\n",
    "    sinknode: str\n",
    "\n",
    "def sv_det(line):\n",
    "    \"\"\"\n",
    "    Determines the SV types per line\n",
    "    Input: Each line of the gfatools bubble output\n",
    "    Output: Insertions or Deletions SV based on comparison between ref and non-ref allele\n",
    "    \"\"\"\n",
    "    # sample line: chr01 11093 11118 4 2 s1,s191853,s2,s3\n",
    "    chromo, pos, end, nodes, paths, nodelist = line.strip().split() # added end\n",
    "    bubble = nodelist.split(\",\")[1:-1] # take nodes except first(source) and last(sink)\n",
    "    sourcenode = nodelist.split(\",\")[0]\n",
    "    sinknode = nodelist.split(\",\")[-1]\n",
    "    if nodes in [\"3\",\"4\"]:\n",
    "        if nodes == \"3\": #either Insertion or Deletion\n",
    "            rrank, nodelen = nodeinf[bubble[0]] # nodeinf refer to above cell\n",
    "            if rrank > 0: # if rank of the first node inside the bubble is not the reference rank (means reference has len 0)\n",
    "                svtype = \"Insertion\"\n",
    "                reflen = 0\n",
    "                nonreflen = nodelen\n",
    "                bubnonref = bubble[0]\n",
    "                bubref = 0 # 0 because no reference node\n",
    "            else: # rank is on reference (means non-ref path has len 0)\n",
    "                svtype = \"Deletion\"\n",
    "                reflen = nodelen\n",
    "                nonreflen = 0\n",
    "                bubnonref = 0\n",
    "                bubref = bubble[0]\n",
    "        elif nodes == \"4\":\n",
    "            if bubble[0] == bubble[1]: # typical cases are inversions so do not loop around the bubble\n",
    "                return None\n",
    "            for bub in bubble:\n",
    "                rrank, nodelen = nodeinf[bub]\n",
    "                if rrank > 0:\n",
    "                    nonreflen = nodelen\n",
    "                    bubnonref = bub\n",
    "                else:\n",
    "                    reflen = nodelen\n",
    "                    bubref = bub\n",
    "            svtype = \"AltDel\" if nonreflen < reflen else \"AltIns\"\n",
    "        return chromo, pos, end, svtype, reflen, nonreflen, sourcenode, bubref, bubnonref, sinknode # added end\n",
    "    else:\n",
    "        return None # for biallelic paths with several nodes ; in this analysis, they are usually inversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f6b26e-bec8-401c-9391-1bff017cd7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used in the sv_det function\n",
    "# nodeinf is declared globally\n",
    "# input path: '/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/Trial_4/coverage_add_edge'\n",
    "nodeinf = {}\n",
    "\n",
    "with open('/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/Trial_4/coverage_add_edge/01_combined_coverage_test.tsv') as infile:\n",
    "    next(infile)\n",
    "    for line in infile:\n",
    "        line_comp = line.strip().split()\n",
    "        nodeid = line_comp[0]\n",
    "        nodelen = line_comp[1]\n",
    "        chromo = line_comp[2]\n",
    "        pos = line_comp[3]\n",
    "        rrank = line_comp[4]\n",
    "        nodeinf[nodeid] = [int(rrank), int(nodelen)] # key is nodeid, values are rank and the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43237615-701f-4db0-9184-5c32d255a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input path: \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/Trial_4/crysnanto_bubble/asm5.nip.biallelic.bubble.tsv\"\n",
    "# with open(\"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/Trial_4/crysnanto_bubble/asm5.nip.biallelic.bubble.tsv\") as infile:\n",
    "#    for line in infile:\n",
    "#        if sv_det(line):\n",
    "#            print(*sv_det(line)) # it'll be better if the end coordinate is also included"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043c8a6-9a87-42c0-bc7d-b343d2194404",
   "metadata": {},
   "source": [
    "Path tracing. Novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c078f3b4-bfcc-42b7-bf04-6b7fbbd77361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "'''\n",
    "It reads the biallelic_file using the process_biallelic function and parses the source, refnode, nonrefNode, and sink to \n",
    "generate the possible paths. These are stored in refPath or altPath. Then, a blank column is added, refAsms and altAsms. \n",
    "\n",
    "'''\n",
    "def process_biallelic(biallelic_file): # to process the possible paths\n",
    "    biallelic_df = pd.read_csv(biallelic_file, sep=' ', header=None, \n",
    "                               names = ['chr','start','end','type','refLen','nonrefLen','source','refNode','nonrefNode','sink'])\n",
    "    # Create a new dataframe with the specified columns\n",
    "    new_df = biallelic_df[['chr','start','end','type','refLen','nonrefLen']].copy()\n",
    "    \n",
    "    # Create refPath\n",
    "    new_df['refPath'] = biallelic_df.apply(lambda row: \n",
    "        f\"{row['source']},{row['refNode']},{row['sink']}\" if row['refNode'] != '0' \n",
    "        else f\"{row['source']},{row['sink']}\", axis=1)\n",
    "    \n",
    "    # Create altPath\n",
    "    new_df['altPath'] = biallelic_df.apply(lambda row: \n",
    "        f\"{row['source']},{row['nonrefNode']},{row['sink']}\" if row['nonrefNode'] != '0' \n",
    "        else f\"{row['source']},{row['sink']}\", axis=1)\n",
    "    \n",
    "    # Add empty columns for refAsms and altAsms\n",
    "    new_df['refAsms'] = ''\n",
    "    new_df['altAsms'] = ''\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# helper functions\n",
    "def clean_node(node):\n",
    "    return re.sub(r'[<>]','',node) # inside the bubble, nodes are separated by their directions\n",
    "\n",
    "def parse_info(info):\n",
    "    parts = info.split(':')\n",
    "    if len(parts) < 2: # a fix to address assembles that do not traverse any path through bubbles. These are represented by \".\" on bubble\n",
    "        return None, None    \n",
    "    nodes = tuple(clean_node(node) for node in re.findall(r'[<>]?s\\d+', parts[0]))\n",
    "    length = int(parts[1])\n",
    "    \n",
    "    return nodes, length\n",
    "\n",
    "# to parse the bubbles before processing it\n",
    "def clean_bubble(bubble_file):\n",
    "    # read the files\n",
    "    bubble_df = pd.read_csv(bubble_file, sep='\\t', header=None, \n",
    "                            names = ['chr','start','end','source','sink','info'])\n",
    "    # parse the 'chr' column to remove the id=Nipponbare| before merging\n",
    "    bubble_df['chr'] = bubble_df['chr'].str.split('|').str[1] \n",
    "    \n",
    "    # parse the info column\n",
    "    parsed_info = bubble_df['info'].apply(parse_info)\n",
    "    bubble_df['node'] = parsed_info.apply(lambda x: x[0] if x is not None else None)\n",
    "    bubble_df['len'] = parsed_info.apply(lambda x: x[1] if x is not None else None)\n",
    "\n",
    "    return bubble_df\n",
    "\n",
    "def merge_and_update_asms(biallelic_df, bubbles_dict):\n",
    "    biallelic_df = process_biallelic(biallelic_df)\n",
    "    \n",
    "    def parse_path(path):\n",
    "        return tuple(clean_node(node.strip()) for node in path.split(','))\n",
    "\n",
    "    for asm, bubble_file in bubbles_dict.items():\n",
    "        bubble_df = clean_bubble(bubble_file)\n",
    "        temp_df = pd.merge(biallelic_df, bubble_df, on=['chr','start','end'], suffixes = ('_x','_y'))\n",
    "\n",
    "        temp_dict = {}\n",
    "        for _, row in temp_df.iterrows():\n",
    "            if row['node'] is None or row['len'] is None:\n",
    "                continue\n",
    "\n",
    "            ref_path = parse_path(row['refPath'])\n",
    "            alt_path = parse_path(row['altPath'])\n",
    "            y_path = (clean_node(row['source']),) + row['node'] + (clean_node(row['sink']),)\n",
    "\n",
    "            if y_path == ref_path and row['len'] == row['refLen']:\n",
    "                temp_dict.setdefault(row.name, {'refAsms': '', 'altAsms': ''})['refAsms'] = asm\n",
    "\n",
    "            if y_path == alt_path and row['len'] == row['nonrefLen']:\n",
    "                temp_dict.setdefault(row.name, {'refAsms': '', 'altAsms':''})['altAsms'] = asm\n",
    "        for index, asms in temp_dict.items():\n",
    "            if asms['refAsms']:\n",
    "                biallelic_df.at[index, 'refAsms'] = f\"{biallelic_df.at[index, 'refAsms']},{asms['refAsms']}\" if biallelic_df.at[index, 'refAsms'] else asms['refAsms']\n",
    "            if asms['altAsms']:\n",
    "                biallelic_df.at[index, 'altAsms'] = f\"{biallelic_df.at[index, 'altAsms']},{asms['altAsms']}\" if biallelic_df.at[index, 'altAsms'] else asms['altAsms']\n",
    "    biallelic_df['refAsms'] = biallelic_df['refAsms'].str.rstrip(',')\n",
    "    biallelic_df['altAsms'] = biallelic_df['altAsms'].str.rstrip(',')\n",
    "\n",
    "    return biallelic_df\n",
    "\n",
    "# Usage\n",
    "biallelic = \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/Trial_4/crysnanto_bubble/asm5.nip.biallelic_sv.tsv\"\n",
    "bubbles_dict = {\n",
    "    \"IRGSP\": \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/IRGSP.alleles.bed\", \n",
    "    \"nh232\": \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/nh232.alleles.bed\",\n",
    "    \"cw02\": \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/cw02.alleles.bed\",\n",
    "    \"nh236\": \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/nh236.alleles.bed\",\n",
    "    \"nh286\": \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/nh286.alleles.bed\",\n",
    "    \"nh273\": \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/nh273.alleles.bed\"\n",
    "}\n",
    "\n",
    "result = merge_and_update_asms(biallelic, bubbles_dict)\n",
    "# result.to_csv('test_path_biallelic.tsv', sep = '\\t', header = None, index = False)\n",
    "\n",
    "# test_IRGSP = merge_and_update_asms(biallelic, bubbles_dict)\n",
    "# test_IRGSP['refAsms'].value_counts()['IRGSP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281a3cb4-241c-469c-930a-d0266534e9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 3925 20795 23124 32907 33124\n"
     ]
    }
   ],
   "source": [
    "a_irgsp = result['altAsms'].str.count('IRGSP').sum()\n",
    "b_nh232 = result['altAsms'].str.count('nh232').sum()\n",
    "c_cw02 = result['altAsms'].str.count('cw02').sum()\n",
    "d_nh236 = result['altAsms'].str.count('nh236').sum()\n",
    "e_nh286 = result['altAsms'].str.count('nh286').sum()\n",
    "f_nh273 = result['altAsms'].str.count('nh273').sum()\n",
    "\n",
    "print(a_irgsp,b_nh232,c_cw02,d_nh236,e_nh286,f_nh273)\n",
    "# 17 3925 20795 23124 32907 33124 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "50655c23-c5a1-44fd-9d2f-85434a081c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1465"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nh232 = \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/nh232.alleles.bed\"\n",
    "cw02 = \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/cw02.alleles.bed\"\n",
    "nh236 = \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/nh236.alleles.bed\"\n",
    "nh286 = \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/nh286.alleles.bed\"\n",
    "nh273 = \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/nh273.alleles.bed\"\n",
    "bia = process_biallelic(biallelic)\n",
    "bub = clean_bubble(nh273)\n",
    "bia_nh232 = pd.merge(bia, bub, on=['chr','start','end'], suffixes = ('_x','_y'))\n",
    "(bia_nh232['info'] == \".\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "bf861eb1-5497-43ef-8a75-0960ce3fed6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refAsms\n",
      "0       17\n",
      "1    61089\n",
      "Name: count, dtype: int64 refAsms\n",
      "0     4181\n",
      "1    56925\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "a_irgsp = result['refAsms'].str.count('IRGSP').value_counts().sort_index()\n",
    "b_nh232 = result['refAsms'].str.count('nh232').value_counts().sort_index()\n",
    "c_cw02 = result['refAsms'].str.count('cw02').value_counts().sort_index()\n",
    "d_nh236 = result['refAsms'].str.count('nh236').value_counts().sort_index()\n",
    "print (a_irgsp, b_nh232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9ab37682-3904-42bd-843c-6484c3bbb503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61089"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import argparse\n",
    "\n",
    "def process_biallelic(biallelic_file): # to process the possible paths\n",
    "    biallelic_df = pd.read_csv(biallelic_file, sep=' ', header=None)\n",
    "    biallelic_df.columns = ['chr','start','end','type','refLen','nonrefLen','source','refNode','nonrefNode','sink']\n",
    "    # Create a new dataframe with the specified columns\n",
    "    new_df = biallelic_df[['chr','start','end','type','refLen','nonrefLen']].copy()\n",
    "    # Create refPath\n",
    "    new_df['refPath'] = biallelic_df.apply(lambda row: \n",
    "        f\"{row['source']},{row['refNode']},{row['sink']}\" if row['refNode'] != '0' \n",
    "        else f\"{row['source']},{row['sink']}\", axis=1)\n",
    "    # Create altPath\n",
    "    new_df['altPath'] = biallelic_df.apply(lambda row: \n",
    "        f\"{row['source']},{row['nonrefNode']},{row['sink']}\" if row['nonrefNode'] != '0' \n",
    "        else f\"{row['source']},{row['sink']}\", axis=1)\n",
    "    # Add empty columns for refAsms and altAsms\n",
    "    new_df['refAsms'] = ''\n",
    "    new_df['altAsms'] = ''\n",
    "    return new_df\n",
    "\n",
    "def clean_bubble(irgsp_file):\n",
    "    # Read the files\n",
    "    irgsp_df = pd.read_csv(irgsp_file, sep='\\t', header=None)\n",
    "    # Rename columns for clarity\n",
    "    irgsp_df.columns = ['chr','start','end','source','sink','info']\n",
    "    # Parse the 'id' column to remove \"id=Nipponbare|\" before merging\n",
    "    irgsp_df['chr'] = irgsp_df['chr'].str.split('|').str[1]\n",
    "    return irgsp_df\n",
    "\n",
    "def update_asms(df_x, df_y,asms):\n",
    "    df_x = process_biallelic(df_x)\n",
    "    df_y = clean_bubble(df_y)\n",
    "    merged_df = pd.merge(df_x, df_y, on=['chr', 'start', 'end'], suffixes=('_x', '_y'))\n",
    "    def clean_node(node):\n",
    "        return re.sub(r'[<>]','',node)\n",
    "        \n",
    "    def parse_path(path):\n",
    "        return tuple(clean_node(node.strip()) for node in path.split(','))\n",
    "        \n",
    "    def parse_info(info):\n",
    "        parts = info.split(':')\n",
    "        nodes = tuple(clean_node(node) for node in re.findall(r'[<>]?s\\d+', parts[0]))\n",
    "        length = int(parts[1])\n",
    "        return nodes, length   \n",
    " \n",
    "    for index, row in merged_df.iterrows():\n",
    "        ref_path = parse_path(row['refPath'])\n",
    "        alt_path = parse_path(row['altPath'])\n",
    "            \n",
    "        bubble_nodes, y_length = parse_info(row['info'])\n",
    "        y_path = (clean_node(row['source']),) + bubble_nodes + (clean_node(row['sink']),)\n",
    "            \n",
    "        if y_path == ref_path and y_length == row['refLen']:\n",
    "            df_x.at[index, 'refAsms'] += f\"{asms},\" if df_x.at[index, 'refAsms'] else asms\n",
    "            \n",
    "        if y_path == alt_path and y_length == row['nonrefLen']:\n",
    "            df_x.at[index, 'altAsms'] += f\"{asms},\" if df_x.at[index, 'altAsms'] else asms\n",
    "    \n",
    "    # Remove trailing commas if any\n",
    "    df_x['refAsms'] = df_x['refAsms'].str.rstrip(',')\n",
    "    df_x['altAsms'] = df_x['altAsms'].str.rstrip(',')\n",
    "    \n",
    "    return df_x\n",
    "\n",
    "# Usage\n",
    "biallelic = \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/Trial_4/crysnanto_bubble/asm5.nip.biallelic_sv.tsv\"\n",
    "irgsp = \"/Users/jongpaduhilao/Desktop/LAB_Files/Initial_Pangenome_analysis/bubble/04_bubble/IRGSP.alleles.bed\"\n",
    "\n",
    "result = update_asms(biallelic, irgsp, 'IRGSP')\n",
    "result['refAsms'].value_counts()['IRGSP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5ddcac97-6241-4f20-8bbd-40c69d1438e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRGSP_path.tsv               \u001b[34mcoverage_add_edge\u001b[m\u001b[m/\n",
      "\u001b[34mRaw_data\u001b[m\u001b[m/                    \u001b[34mcrysnanto_bubble\u001b[m\u001b[m/\n",
      "SV_analysis_polishing.ipynb  \u001b[34mmgutils_study\u001b[m\u001b[m/\n",
      "SV_analysis_test.ipynb       test_path_biallelic.tsv\n",
      "\u001b[34mScripts\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e0934ab6-ce52-4a18-bd8f-a38936730562",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = pd.read_csv(\"test_path_biallelic.tsv\", sep = '\\t', header = None,\n",
    "                        names = ['chr','start','end','type','refLen','altLen','refNodes','altNodes','refAsms','altAsms']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "7f5c20e1-b05c-46d6-bc4a-29f24d38b8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    IRGSP,nh232,nh236\n",
       "1               IRGSP,nh232,cw02,nh236\n",
       "2        IRGSP,nh232,nh236,nh286,nh273\n",
       "3                    IRGSP,nh232,nh236\n",
       "4               IRGSP,nh232,cw02,nh236\n",
       "                     ...              \n",
       "61101                      IRGSP,nh232\n",
       "61102     IRGSP,nh232,cw02,nh236,nh286\n",
       "61103     IRGSP,nh232,cw02,nh286,nh273\n",
       "61104           IRGSP,nh232,cw02,nh236\n",
       "61105    IRGSP,nh232,nh236,nh286,nh273\n",
       "Name: refAsms, Length: 61106, dtype: object"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path['refAsms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "44c3795a-518b-47ce-b24f-e7bc4d3218db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = test_path['refAsms'].str.split(',').apply(lambda x: x == \"IRGSP\").sum()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0fe80-6776-47b3-adf8-387c89972310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
